{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "F6wIayRLk6vx"
      },
      "outputs": [],
      "source": [
        "import torch, json\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('Sample_Data.json') as f:\n",
        "    d = json.load(f)"
      ],
      "metadata": {
        "id": "U0cCJpptmvXd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract data points into arrays\n",
        "#timestamps   = []; ## we may not need this for now\n",
        "gt_locations = [];\n",
        "inp_rss_vals = [];\n",
        "for datapoint in d[\"data\"]:\n",
        "    #timestamps.append(datapoint[\"timestamp\"])\n",
        "    gt_locations.append( [ float(datapoint[\"location\"][\"loc_x\"]),\n",
        "                           float(datapoint[\"location\"][\"loc_y\"]) ]) # keeping x and y together in 2x1 vector\n",
        "\n",
        "    inp_rss_vals.append( [ float(datapoint[\"signal_strength\"][\"RSS_1\"]) ,\n",
        "                           float(datapoint[\"signal_strength\"][\"RSS_2\"]) ,\n",
        "                           float(datapoint[\"signal_strength\"][\"RSS_3\"]) ] )\n",
        "\n",
        "gt_locations = np.asarray(gt_locations)\n",
        "inp_rss_vals = np.asarray(inp_rss_vals)"
      ],
      "metadata": {
        "id": "xHea-zHSnFUX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(gt_locations.shape)\n",
        "print(inp_rss_vals.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQZv52wgommW",
        "outputId": "3305c991-b9ed-421a-923c-ba0e700cc139"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 2)\n",
            "(2, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_training_iters = 401;\n",
        "batch_size = 1; # this is =1 because we have just one sample, when we have hundreds of samples we can use larger batch sizes"
      ],
      "metadata": {
        "id": "77qX2HEVowqo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# traditionally, inputs are called x and outputs are called y, I'm just doing dummy assignments here to keep the tradition going\n",
        "x_train = inp_rss_vals[0:1,:] # we have 2 data points, let's use 1 for training and 1 for testing\n",
        "x_test  = inp_rss_vals[1:2,:]\n",
        "y_train = gt_locations[0:1,:]\n",
        "y_test  = gt_locations[1:2,:]"
      ],
      "metadata": {
        "id": "9R8a7CYspFb_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5quwKq7PpzCz",
        "outputId": "625c807a-033c-4006-ec2a-bdc3a2b6a4ff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 3) (1, 2) (1, 3) (1, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we need to convert the types of these arrays from numpy to torch and create iterators called \"dataloaders\"\n",
        "tensor_x_train = torch.tensor(x_train).float()\n",
        "tensor_y_train = torch.tensor(y_train).float()\n",
        "tensor_x_test = torch.tensor(x_test).float()\n",
        "tensor_y_test = torch.tensor(y_test).float()\n",
        "\n",
        "# these dataloaders take the whole dataset, randomize their indices if shuffle=True, and then efficiently\n",
        "# load them during training (it's inefficient to loop through data points and choosing random ones in python ourselves)\n",
        "datasets = torch.utils.data.TensorDataset(tensor_x_train, tensor_y_train)\n",
        "train_iter = torch.utils.data.DataLoader(datasets, batch_size=batch_size, shuffle=True)\n",
        "datasets = torch.utils.data.TensorDataset(tensor_x_test, tensor_y_test)\n",
        "test_iter = torch.utils.data.DataLoader(datasets, batch_size=y_test.shape[0], shuffle=False)"
      ],
      "metadata": {
        "id": "pt0-FvTso1zv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this defines the model (it's a simple MLP)\n",
        "class mdl(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(mdl, self).__init__()\n",
        "        self.input_layer    = nn.Linear(3, 16)\n",
        "        self.hidden_layer1  = nn.Linear(16, 32)\n",
        "        self.hidden_layer2  = nn.Linear(32, 20)\n",
        "        self.output_layer   = nn.Linear(20, 2)\n",
        "        self.activation_fcn = nn.ReLU()\n",
        "    def forward(self, x):\n",
        "        x = self.activation_fcn(self.input_layer(x))\n",
        "        x = self.activation_fcn(self.hidden_layer1(x))\n",
        "        x = self.activation_fcn(self.hidden_layer2(x))\n",
        "        x = self.output_layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "PgE0MoCmmXyJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = mdl()\n",
        "model.train()\n",
        "\n",
        "# we optimize for minimizing MSE loss, with an Adam optimizer (a certain flavor of the gradient descent optimizer)\n",
        "criterion = nn.MSELoss(reduction='mean')\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "r0s-ZcyhmmjJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(number_of_training_iters):\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_iter:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        #print(loss)\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    if i % 20 == 0:\n",
        "        print('Epoch [%d]/[%d] running accumulative loss across all batches: %.3f' %\n",
        "              (i + 1, number_of_training_iters, running_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvQfvpzDmoAA",
        "outputId": "0812fdb4-ac94-46bf-bc4a-f2364d9f9897"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1]/[401] running accumulative loss across all batches: 110813.562\n",
            "Epoch [21]/[401] running accumulative loss across all batches: 105717.000\n",
            "Epoch [41]/[401] running accumulative loss across all batches: 94335.500\n",
            "Epoch [61]/[401] running accumulative loss across all batches: 72227.641\n",
            "Epoch [81]/[401] running accumulative loss across all batches: 38858.031\n",
            "Epoch [101]/[401] running accumulative loss across all batches: 11136.074\n",
            "Epoch [121]/[401] running accumulative loss across all batches: 1157.345\n",
            "Epoch [141]/[401] running accumulative loss across all batches: 29.860\n",
            "Epoch [161]/[401] running accumulative loss across all batches: 17.387\n",
            "Epoch [181]/[401] running accumulative loss across all batches: 0.929\n",
            "Epoch [201]/[401] running accumulative loss across all batches: 0.213\n",
            "Epoch [221]/[401] running accumulative loss across all batches: 0.020\n",
            "Epoch [241]/[401] running accumulative loss across all batches: 0.001\n",
            "Epoch [261]/[401] running accumulative loss across all batches: 0.001\n",
            "Epoch [281]/[401] running accumulative loss across all batches: 0.000\n",
            "Epoch [301]/[401] running accumulative loss across all batches: 0.000\n",
            "Epoch [321]/[401] running accumulative loss across all batches: 0.000\n",
            "Epoch [341]/[401] running accumulative loss across all batches: 0.000\n",
            "Epoch [361]/[401] running accumulative loss across all batches: 0.000\n",
            "Epoch [381]/[401] running accumulative loss across all batches: 0.000\n",
            "Epoch [401]/[401] running accumulative loss across all batches: 0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "loss quickly went to 0 because we optimized the model for a single data point, it memorized that data point"
      ],
      "metadata": {
        "id": "j3iLB8DFqme_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_locations_trainset = model(tensor_x_train)\n",
        "print(predicted_locations_trainset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_eWJhYlqmJl",
        "outputId": "b2461f37-4ee8-4725-8ad5-9090c85b28a9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[123.0000, 456.0000]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(gt_locations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-0r3ZF_q6qm",
        "outputId": "32b89241-be02-4211-cf33-619e3ebb9d12"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[123. 456.]\n",
            " [234. 567.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "perfect output! Since the model memorized this though, it will possibly not do this well on the test set, let's see this:"
      ],
      "metadata": {
        "id": "HY5cIBfpq_JK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_locations_test = model(tensor_x_test)\n",
        "print(predicted_locations_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkkhYBAerDAk",
        "outputId": "5e483591-97bf-4bc8-be54-daac55ecffa3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[126.7971, 470.1260]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "see how it's still thinking the location is something like 123 by 456? This is overfitting, and it's expected because we have a single data point!\n",
        "\n",
        "this will (hopefully) not happen in our larger dataset"
      ],
      "metadata": {
        "id": "Oct9ePGVrSLn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pJAHIpCHmrbf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}